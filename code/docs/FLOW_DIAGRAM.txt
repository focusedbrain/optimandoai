# Ollama Diagnostic & Auto-Mitigation Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                      APP STARTUP                                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  1. Initialize OllamaManager                                     │
│     await ollamaManager.initialize()                             │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  2. Run Hardware Diagnostics                                     │
│     hardwareDiagnostics.diagnose()                               │
│                                                                   │
│     ┌───────────────────────────────────────────┐              │
│     │ Detect CPU (cores, model, physical)      │              │
│     │ Detect RAM (total, free)                 │              │
│     │ Detect GPU (vendor, name, integrated)    │              │
│     │ Check Vulkan (vulkaninfo --summary)      │              │
│     └───────────────────────────────────────────┘              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  3. Generate Recommendations                                     │
│                                                                   │
│     IF Vulkan healthy AND discrete GPU:                         │
│        → Use GPU mode                                            │
│     ELSE:                                                        │
│        → Use CPU mode (set OLLAMA_NO_GPU=1)                     │
│                                                                   │
│     IF RAM < 8GB:                                                │
│        → ctx=512, batch=8, threads=2, quant=q2_K                │
│     ELSE IF RAM < 16GB:                                          │
│        → ctx=1024, batch=16, threads=4, quant=q4_K_M            │
│     ELSE:                                                        │
│        → ctx=2048, batch=32, threads=4, quant=q4_K_M            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  4. Start Ollama Server                                          │
│     ollamaManager.start()                                        │
│                                                                   │
│     ENV = { OLLAMA_NO_GPU: cpuFallbackMode ? '1' : undefined } │
│     spawn('ollama', ['serve'], { env })                          │
│                                                                   │
│     Wait for server ready (30s timeout)                         │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
                    ┌─────────────────┐
                    │  Server Failed?  │
                    └─────────────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
                   YES                 NO
                    │                   │
                    ▼                   ▼
        ┌───────────────────┐  ┌──────────────┐
        │ Already CPU mode? │  │   SUCCESS    │
        └───────────────────┘  │  Log Health  │
                    │           │   Status     │
          ┌─────────┴─────┐    └──────────────┘
          │               │
         YES             NO
          │               │
          ▼               ▼
    ┌─────────┐  ┌──────────────┐
    │  FAIL   │  │ Retry w/ CPU │
    │  throw  │  │     mode     │
    └─────────┘  └──────────────┘
                         │
                         └──(back to start)


┌─────────────────────────────────────────────────────────────────┐
│                   USER SENDS CHAT MESSAGE                        │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  5. Chat with Model                                              │
│     ollamaManager.chat(modelId, messages)                        │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  6. Apply Safe Options (from diagnostics)                        │
│                                                                   │
│     options = {                                                  │
│       num_ctx: diagnostics.recommendations.maxContext,          │
│       num_batch: diagnostics.recommendations.maxBatch,          │
│       num_thread: diagnostics.recommendations.numThreads        │
│     }                                                            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  7. Start Watchdog Timer (90 seconds)                            │
│                                                                   │
│     Promise.race([                                               │
│       actualChatRequest,                                         │
│       watchdogTimeout(90000)                                     │
│     ])                                                           │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  8. Execute Chat Request                                         │
│     POST http://127.0.0.1:11434/api/chat                        │
│     {                                                            │
│       model: modelId,                                            │
│       messages: [...],                                           │
│       options: { num_ctx, num_batch, num_thread }               │
│     }                                                            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
                    ┌─────────────────┐
                    │  Request OK?     │
                    └─────────────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
                   YES                 NO
                    │                   │
                    ▼                   ▼
        ┌──────────────────┐  ┌────────────────────┐
        │  Clear Watchdog  │  │  Request Failed    │
        │  Return Response │  │  or Timed Out      │
        │  Log Duration    │  └────────────────────┘
        └──────────────────┘            │
                                        ▼
                              ┌──────────────────────┐
                              │ Fallback Attempted?  │
                              └──────────────────────┘
                                        │
                              ┌─────────┴─────────┐
                              │                   │
                             YES                 NO
                              │                   │
                              ▼                   ▼
                  ┌────────────────────┐  ┌──────────────────┐
                  │ Convert to User-   │  │ Enable CPU Mode  │
                  │ Friendly Error     │  │ Restart Ollama   │
                  │ Return to User     │  │ Retry Chat       │
                  └────────────────────┘  └──────────────────┘
                                                    │
                                                    └──(back to step 5)


┌─────────────────────────────────────────────────────────────────┐
│                      ERROR CONVERSION                            │
└─────────────────────────────────────────────────────────────────┘

"timeout" / "hung"
    ↓
"Model loading timed out. Try:
 1. Using a smaller model (TinyLlama)
 2. Closing other applications
 3. Restarting the application"

"GPU" / "Vulkan" / "CUDA"
    ↓
"GPU/graphics driver issue detected.
The app has switched to CPU-only mode.
For better performance, update your graphics drivers."

"out of memory" / "OOM"
    ↓
"Insufficient memory. Try:
 1. Using a smaller model
 2. Closing other applications
 3. Restarting your computer"

"model not found" / "404"
    ↓
"Model not found.
Please install it from LLM Settings first."


┌─────────────────────────────────────────────────────────────────┐
│                         LOGGING                                  │
└─────────────────────────────────────────────────────────────────┘

Every operation logged to:
  %USERPROFILE%\AppData\Roaming\<App>\logs\ollama-debug.log

Format:
  [2025-11-21T10:30:15.123Z] [LEVEL] [Category] Message {data}

Rotation:
  - Max file size: 5MB
  - Max files: 3
  - Total max: 15MB

Examples:
  [INFO] [HardwareDiagnostics] CPU: Intel Core i5-7200U
  [WARN] [OllamaManager] Starting in CPU-only mode
  [INFO] [OllamaManager] Chat completed (duration: 5600ms)
  [ERROR] [OllamaManager] Chat failed, attempting fallback


┌─────────────────────────────────────────────────────────────────┐
│                    USER EXPERIENCE                               │
└─────────────────────────────────────────────────────────────────┘

Weak Hardware User:
  1. Installs app
  2. App detects: Intel HD 4000, 4GB RAM
  3. Auto-switches to CPU mode
  4. Installs TinyLlama automatically
  5. Sends first message
  6. Receives response in 10-15 seconds (slow but stable)
  7. ✅ No freezes, no crashes, no manual config

Strong Hardware User:
  1. Installs app
  2. App detects: RTX 3060, 16GB RAM
  3. Uses GPU mode
  4. Installs Phi-3
  5. Sends first message
  6. Receives response in 2-3 seconds
  7. ✅ Fast performance

User with Broken GPU:
  1. Installs app
  2. App detects: NVIDIA GPU, but Vulkan broken
  3. Auto-switches to CPU mode (logs: "Vulkan unhealthy")
  4. Chat works (slower than GPU, but stable)
  5. User sees: "⚠️ Running in CPU-only mode"
  6. ✅ No crashes, clear explanation


┌─────────────────────────────────────────────────────────────────┐
│                     KEY FEATURES                                 │
└─────────────────────────────────────────────────────────────────┘

✅ Zero Configuration
   - Automatic hardware detection
   - Automatic fallback
   - No user intervention needed

✅ Robust Fallback
   - GPU → CPU → Reduced Resources
   - Transparent to user
   - Logged for debugging

✅ Hang Protection
   - 90-second watchdog
   - Prevents infinite hangs
   - App remains responsive

✅ User-Friendly Errors
   - Technical → Actionable
   - Suggests solutions
   - Links to docs

✅ Comprehensive Logging
   - All decisions logged
   - Rotating files (15MB max)
   - Easy debugging


┌─────────────────────────────────────────────────────────────────┐
│                   PERFORMANCE IMPACT                             │
└─────────────────────────────────────────────────────────────────┘

Startup:       +1-2 seconds (one-time diagnostics)
First Chat:    Same or faster (prevents hangs)
Later Chats:   No difference
Memory:        +5MB (cached diagnostics)
Disk:          ~15MB (log files)

Trade-off:
  Small startup cost for:
  - Preventing crashes
  - Preventing hangs
  - Better user experience
  - Easier debugging

Result: NET POSITIVE ✅
```





